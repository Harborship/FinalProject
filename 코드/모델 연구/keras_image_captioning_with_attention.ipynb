{"cells":[{"cell_type":"markdown","metadata":{"id":"y13sF4m4gaew"},"source":["[참고] : https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18788,"status":"ok","timestamp":1657169709684,"user":{"displayName":"groupfive1 multi","userId":"14653379220115847940"},"user_tz":-540},"id":"XOTGK3FdeHUl","outputId":"ff697d78-b0b3-4580-bd03-6acbc8344a6a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4712,"status":"ok","timestamp":1657169714391,"user":{"displayName":"groupfive1 multi","userId":"14653379220115847940"},"user_tz":-540},"id":"DrJzhKC9BcfB","outputId":"6b281973-3015-4ac1-ff89-3b44aa3944f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting visual-attention-tf\n","  Downloading visual_attention_tf-1.2.0-py3-none-any.whl (5.4 kB)\n","Requirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from visual-attention-tf) (2.8.2+zzzcolab20220527125636)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (3.1.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (3.3.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (2.8.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (0.26.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (2.8.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.14.1)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.21.6)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.6.3)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (14.0.1)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.1.2)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (0.2.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.46.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (4.1.1)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (3.17.3)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.15.0)\n","Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (2.8.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (0.5.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (57.4.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (1.1.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.2.0->visual-attention-tf) (2.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->visual-attention-tf) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.2.0->visual-attention-tf) (1.5.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (0.4.6)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (3.3.7)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (1.8.1)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (0.6.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (2.23.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (0.4.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (2022.6.15)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.2.0->visual-attention-tf) (3.2.0)\n","Installing collected packages: visual-attention-tf\n","Successfully installed visual-attention-tf-1.2.0\n"]}],"source":["!pip install visual-attention-tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2xzhOTSUhK43"},"outputs":[],"source":["import numpy as np\n","from numpy import array\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import os\n","from PIL import Image\n","from keras.layers import Dense, Activation, Flatten, Reshape, Dropout, Conv2D, Conv1D, GlobalAveragePooling2D\n","from tensorflow.keras.optimizers import Adam, RMSprop\n","from keras.layers.merge import add\n","from keras.applications.inception_v3 import InceptionV3\n","from keras.applications.inception_v3 import preprocess_input\n","from tensorflow.keras.utils import img_to_array, custom_object_scope\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n","import keras\n","from keras.models import Model\n","from keras import Input, layers\n","from tqdm import tqdm\n","import tensorflow as tf\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4RWjJKZu6D7"},"outputs":[],"source":["class EfficientChannelAttention2D(tf.keras.layers.Layer):\n","\n","    def __init__(self, nf=32, **kwargs):\n","        super(EfficientChannelAttention2D, self).__init__(**kwargs)\n","        self.nf = nf\n","        self.conv1 = Conv1D(filters=1, kernel_size=3, activation=None,padding=\"same\", use_bias=False)\n","\n","    @tf.function\n","    def call(self, x):\n","        pool = tf.reduce_mean(x,[1,2])\n","        pool = tf.expand_dims(pool,-1)\n","        att = self.conv1(pool) #set k=3 for every channel size between 8 and 64\n","        att = tf.transpose(att,perm=[0,2,1])\n","        att = tf.expand_dims(att,1)\n","        att = tf.sigmoid(att)\n","        y = tf.multiply(x,att)\n","        return y\n","\n","    def get_config(self):\n","        config = super(EfficientChannelAttention2D, self).get_config()\n","        config.update({\"Att_filters\": self.nf})\n","        config = super(EfficientChannelAttention2D, self).get_config()\n","        return config"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lGSUDAf0gibZ"},"outputs":[],"source":["# 압축파일 압축해제 코드\n","from zipfile import ZipFile\n","with ZipFile('/content/drive/MyDrive/5조/데이터/음식.zip', 'r') as zip:\n","    zip.extractall('./temp1')\n","\n","with ZipFile('/content/drive/MyDrive/5조/데이터/반려동물_완.zip', 'r') as zip:\n","    zip.extractall('./temp2')\n","\n","with ZipFile('/content/drive/MyDrive/5조/데이터/육아.zip', 'r') as zip:\n","    zip.extractall('./temp3')\n","\n","with ZipFile('/content/drive/MyDrive/5조/데이터/디저트.zip', 'r') as zip:\n","    zip.extractall('./temp4')\n","\n","with ZipFile('/content/drive/MyDrive/5조/데이터/네일아트_완.zip', 'r') as zip:\n","    zip.extractall('./temp5')\n","\n","with ZipFile('/content/drive/MyDrive/5조/데이터/옷스타그램 (재업).zip', 'r') as zip:\n","    zip.extractall('./temp6')\n"]},{"cell_type":"code","source":["df1=pd.read_csv('/content/drive/MyDrive/5조/데이터/csv/food_info.csv', encoding='cp949') # id가 1000001~1003048\n","column=df1.columns\n","\n","df2=pd.read_csv('/content/drive/MyDrive/5조/데이터/csv/반려동물_info.csv')  # id가 2007781~2012346\n","df2.columns=column\n","\n","df3=pd.read_csv('/content/drive/MyDrive/5조/데이터/csv/육아.csv')   # id가 39000~310159\n","df3=df3[['이미지id', 'url', '음식', '음료수', '성인', '아동', '동물', '네일', '꽃', '기타', '패션']]\n","\n","df4=pd.read_csv('/content/drive/MyDrive/5조/데이터/csv/디저트.csv', encoding='cp949')   # id가 6028883~6035562\n","df4.dropna(inplace=True)\n","df4=df4[['이미지id', 'url', '음식', '음료수', '성인', '아동', '동물', '네일', '꽃', '기타', '패션']]\n","\n","df5=pd.read_csv('/content/drive/MyDrive/5조/데이터/csv/labeled/네일아트_id_url_src_labeld_0707.csv', encoding='cp949')   # id가 6000000~6004522\n","df5.columns= ['이미지id', 'url', 'src', '음식', '음료수', '성인', '아동', '동물', '네일', '꽃', '기타', '패션']\n","df5=df5[['이미지id', 'url', '음식', '음료수', '성인', '아동', '동물', '네일', '꽃', '기타', '패션']]\n","\n","df6=pd.read_csv('/content/drive/MyDrive/5조/데이터/csv/labeled/옷스타그램_id_url_src_0707.csv', encoding='cp949')   # id가 6007781~6028882\n","df6=df6[['이미지id', 'url', '음식', '음료수', '성인', '아동', '동물', '네일', '꽃', '기타', '패션']]\n","\n","df_total=pd.concat([df1, df2, df3, df4, df5, df6])\n","df_total.reset_index(drop=True, inplace=True)\n","df_total.dropna(inplace=True)\n","df_total"],"metadata":{"id":"B1ed7GbVu4_S","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1657169754646,"user_tz":-540,"elapsed":5195,"user":{"displayName":"groupfive1 multi","userId":"14653379220115847940"}},"outputId":"d4f7f6d7-5239-4e9f-ca91-1a797de5a570"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["             이미지id                                       url   음식  음료수   성인  \\\n","0      1000001.jpg  https://www.instagram.com/p/CfP6mTqhoHJ/  1.0  0.0  0.0   \n","1      1000002.jpg  https://www.instagram.com/p/CfTNdZCu1SV/  1.0  0.0  0.0   \n","2      1000003.jpg  https://www.instagram.com/p/CfX4ZwIFUp_/  1.0  0.0  0.0   \n","3      1000004.jpg  https://www.instagram.com/p/CfWMXQpl5tI/  1.0  0.0  0.0   \n","4      1000005.jpg  https://www.instagram.com/p/CfN05YRP4rC/  1.0  0.0  0.0   \n","...            ...                                       ...  ...  ...  ...   \n","37820  6028878.jpg                           /p/CfOQ3G5PK81/  0.0  0.0  0.0   \n","37821  6028879.jpg                           /p/CfLkzONhdVh/  0.0  0.0  0.0   \n","37822  6028880.jpg                           /p/CfB6RLDvO77/  0.0  0.0  1.0   \n","37823  6028881.jpg                           /p/Ce3mdxAvc1l/  1.0  0.0  1.0   \n","37824  6028882.jpg                           /p/CVvDm4UvY0e/  0.0  0.0  1.0   \n","\n","        아동   동물   네일    꽃   기타   패션  \n","0      0.0  0.0  0.0  0.0  0.0  0.0  \n","1      0.0  0.0  0.0  0.0  0.0  0.0  \n","2      0.0  0.0  0.0  0.0  0.0  0.0  \n","3      0.0  0.0  0.0  0.0  0.0  0.0  \n","4      0.0  0.0  0.0  0.0  0.0  0.0  \n","...    ...  ...  ...  ...  ...  ...  \n","37820  0.0  0.0  0.0  0.0  1.0  0.0  \n","37821  0.0  0.0  0.0  0.0  1.0  0.0  \n","37822  0.0  0.0  0.0  0.0  0.0  0.0  \n","37823  0.0  0.0  0.0  0.0  0.0  0.0  \n","37824  0.0  0.0  0.0  0.0  0.0  0.0  \n","\n","[37823 rows x 11 columns]"],"text/html":["\n","  <div id=\"df-2155e3f0-a3fc-4704-a9fe-971ba1fae4a8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>이미지id</th>\n","      <th>url</th>\n","      <th>음식</th>\n","      <th>음료수</th>\n","      <th>성인</th>\n","      <th>아동</th>\n","      <th>동물</th>\n","      <th>네일</th>\n","      <th>꽃</th>\n","      <th>기타</th>\n","      <th>패션</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000001.jpg</td>\n","      <td>https://www.instagram.com/p/CfP6mTqhoHJ/</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000002.jpg</td>\n","      <td>https://www.instagram.com/p/CfTNdZCu1SV/</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1000003.jpg</td>\n","      <td>https://www.instagram.com/p/CfX4ZwIFUp_/</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000004.jpg</td>\n","      <td>https://www.instagram.com/p/CfWMXQpl5tI/</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000005.jpg</td>\n","      <td>https://www.instagram.com/p/CfN05YRP4rC/</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>37820</th>\n","      <td>6028878.jpg</td>\n","      <td>/p/CfOQ3G5PK81/</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>37821</th>\n","      <td>6028879.jpg</td>\n","      <td>/p/CfLkzONhdVh/</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>37822</th>\n","      <td>6028880.jpg</td>\n","      <td>/p/CfB6RLDvO77/</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>37823</th>\n","      <td>6028881.jpg</td>\n","      <td>/p/Ce3mdxAvc1l/</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>37824</th>\n","      <td>6028882.jpg</td>\n","      <td>/p/CVvDm4UvY0e/</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>37823 rows × 11 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2155e3f0-a3fc-4704-a9fe-971ba1fae4a8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2155e3f0-a3fc-4704-a9fe-971ba1fae4a8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2155e3f0-a3fc-4704-a9fe-971ba1fae4a8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJgCeVEXi3ov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657169909591,"user_tz":-540,"elapsed":10617,"user":{"displayName":"groupfive1 multi","userId":"14653379220115847940"}},"outputId":"ecdb5b9d-0988-4960-fed1-020e9ddd3530"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n","96116736/96112376 [==============================] - 3s 0us/step\n","96124928/96112376 [==============================] - 3s 0us/step\n"]}],"source":["# 이미지의 벡터화를 진행하는 코드\n","\n","# Load the inception v3 model\n","model = InceptionV3(weights='imagenet')\n","\n","# Create a new model, by removing the last layer (output layer) from the inception v3\n","model_new = Model(model.input, model.layers[-3].output)\n","\n","# Function to encode a given image into a vector of size (8, 8, 2048)\n","def encode(image_path):\n","    img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n","    # Convert image to numpy array of 3-dimensions\n","    x = img_to_array(img)\n","    # Add one more dimension\n","    x = np.expand_dims(x, axis=0)\n","    # preprocess the images using preprocess_input() from inception module\n","    x = preprocess_input(x)\n","    fea_vec = model_new.predict(x) # Get the encoding vector for the image\n","\n","    return fea_vec   # 이 데이터가 input data로 사용됨"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5kgD5W7iHW-I","colab":{"base_uri":"https://localhost:8080/"},"outputId":"31d1c001-b7f1-4f71-daa7-3d9447f2760e","executionInfo":{"status":"ok","timestamp":1657173923346,"user_tz":-540,"elapsed":4013770,"user":{"displayName":"groupfive1 multi","userId":"14653379220115847940"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 37823/37823 [1:06:34<00:00,  9.47it/s]\n"]},{"output_type":"execute_result","data":{"text/plain":["(37823, 64, 2048)"]},"metadata":{},"execution_count":8}],"source":["image_path1='/content/temp1/'\n","image_path2='/content/temp2/╣▌╖┴╡┐╣░/'\n","image_path3='/content/temp3/└░╛╞/'\n","image_path4='/content/temp4/'\n","image_path5='/content/temp5/'\n","image_path6='/content/temp6/'\n","data=[]\n","for image in tqdm( df_total['이미지id'].values ):\n","  if int(image[0])==1:\n","    path=image_path1+image\n","    data.append( np.array(encode(path)).reshape(64,2048) )\n","  elif int(image[0])==2:\n","    path=image_path2+image\n","    data.append( np.array(encode(path)).reshape(64,2048) )\n","  elif int(image[0])==3:\n","    path=image_path3+image\n","    data.append( np.array(encode(path)).reshape(64,2048) )\n","  elif int(image[0])==6:\n","    if int(image.split('.')[0])<=6035562 and int(image.split('.')[0])>= 6028883:\n","      path=image_path4+image\n","      data.append( np.array(encode(path)).reshape(64,2048) )\n","    elif int(image.split('.')[0])<=6004522 and int(image.split('.')[0])>= 6000000:\n","      path=image_path5+image\n","      data.append( np.array(encode(path)).reshape(64,2048) )\n","    elif int(image.split('.')[0])<=6028882 and int(image.split('.')[0])>= 6007781:\n","      path=image_path6+image\n","      data.append( np.array(encode(path)).reshape(64,2048) )\n","\n","data=np.array(data)\n","data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRyJVE9AxM9T"},"outputs":[],"source":["Y=df_total.iloc[:, 2:].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boauZHlLpcrw"},"outputs":[],"source":["# train_test_split을 이용해 데이터를 분할\n","X_train, X_test, Y_train, Y_test=train_test_split(\n","    data, Y, test_size=0.2, random_state=2022\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCTaZ_Kbi3xb"},"outputs":[],"source":["# image feature extractor model\n","inputs1 = Input(shape=(64, 2048, 1))\n","cnn_layer1 = Conv2D(32, kernel_size=(3,3), input_shape=(64, 2048, 1), activation='relu', padding='same')(inputs1)\n","cnn_layer2 = Conv2D(32, kernel_size=(3,3), input_shape=(64, 2048, 1), activation='relu', padding='same')(cnn_layer1)\n","# Using the .shape[-1] to simplify network modifications. Can directly input number of channels as well\n","attention_cnn = EfficientChannelAttention2D(cnn_layer2.shape[-1])(cnn_layer2)\n","drop=Dropout(0.25)(attention_cnn)\n","pool = GlobalAveragePooling2D()(drop)\n","d1=Dense(128, activation='relu')(pool)\n","d2=Dense(256, activation='relu')(d1)\n","d3=Dense(256, activation='relu')(d2)\n","d4=Dense(512, activation='relu')(d2)\n","outputs = Dense(9, activation='sigmoid')(d4)   # multi label classification 모델이므로 activation이 sigmoid가 되어야 함\n","\n","model = Model(inputs=inputs1, outputs=outputs)\n"]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"8O_c0Zm87ql9","executionInfo":{"status":"ok","timestamp":1657173944858,"user_tz":-540,"elapsed":9,"user":{"displayName":"groupfive1 multi","userId":"14653379220115847940"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d764c812-7c60-43af-a5f1-47fdbdf4fe3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 64, 2048, 1)]     0         \n","                                                                 \n"," conv2d_94 (Conv2D)          (None, 64, 2048, 32)      320       \n","                                                                 \n"," conv2d_95 (Conv2D)          (None, 64, 2048, 32)      9248      \n","                                                                 \n"," efficient_channel_attention  (None, 64, 2048, 32)     3         \n"," 2d (EfficientChannelAttenti                                     \n"," on2D)                                                           \n","                                                                 \n"," dropout (Dropout)           (None, 64, 2048, 32)      0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 32)               0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 128)               4224      \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               33024     \n","                                                                 \n"," dense_3 (Dense)             (None, 512)               131584    \n","                                                                 \n"," dense_4 (Dense)             (None, 9)                 4617      \n","                                                                 \n","=================================================================\n","Total params: 183,020\n","Trainable params: 183,020\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPIhCbbtjHhj"},"outputs":[],"source":["model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model_path='best_model.h5'\n","mc=ModelCheckpoint(model_path, save_best_only=True, verbose=1, save_weights_only=False)\n","es=EarlyStopping(patience=10)"]},{"cell_type":"code","source":["hist=model.fit(\n","    X_train, Y_train, validation_split=0.2, epochs=100, callbacks=[mc,es],  batch_size=80\n",")"],"metadata":{"id":"WSu9YUYlI-GJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hEDoOsCsOtNV"},"outputs":[],"source":["best_model=load_model('best_model.h5' , custom_objects={'EfficientChannelAttention2D' : EfficientChannelAttention2D} )\n","label=best_model.predict(X_test)\n","\n","y_pred=[]\n","for sample in label:\n","  y_pred.append([1 if i>=0.5 else 0 for i in sample ] )\n","y_pred = np.array(y_pred)\n","print(accuracy_score(Y_test, y_pred))\n","\n","real_df=pd.DataFrame(Y_test)\n","pred_df=pd.DataFrame(y_pred)"]},{"cell_type":"code","source":["# 음식에 대한 모델의 예측력\n","from sklearn.metrics import accuracy_score\n","accuracy_score(real_df[0], pred_df[0])"],"metadata":{"id":"dwgr4C2vOpw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 반려동물에 대한 모델의 예측력\n","from sklearn.metrics import accuracy_score\n","accuracy_score(real_df[4], pred_df[4])"],"metadata":{"id":"CBl1nWa7K37M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 아동에 대한 모델의 예측력\n","from sklearn.metrics import accuracy_score\n","accuracy_score(real_df[3], pred_df[3])"],"metadata":{"id":"DvH6sedfMsNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 성인에 대한 모델의 예측력\n","from sklearn.metrics import accuracy_score\n","accuracy_score(real_df[2], pred_df[2])"],"metadata":{"id":"dbMQEDrmOm03"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_p37vlKtbKVu"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"keras_image_captioning_with_attention.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}