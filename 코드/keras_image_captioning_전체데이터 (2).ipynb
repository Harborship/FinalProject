{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y13sF4m4gaew"
      },
      "source": [
        "[참고] : https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOTGK3FdeHUl",
        "outputId": "d37afe80-54f0-4f74-81d5-1f5b2ea13283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2xzhOTSUhK43"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "from time import time\n",
        "from keras.layers import Dense, Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from keras.layers.merge import add\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import keras\n",
        "from keras.models import Model\n",
        "from keras import Input, layers\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lGSUDAf0gibZ"
      },
      "outputs": [],
      "source": [
        "# 압축파일 압축해제 코드\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/음식.zip', 'r') as zip:\n",
        "    zip.extractall('./temp1')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/반려동물_완.zip', 'r') as zip:\n",
        "    zip.extractall('./temp2')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/육아.zip', 'r') as zip:\n",
        "    zip.extractall('./temp3')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/디저트.zip', 'r') as zip:\n",
        "    zip.extractall('./temp4')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/네일아트_완.zip', 'r') as zip:\n",
        "    zip.extractall('./temp5')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/옷스타그램 (재업).zip', 'r') as zip:\n",
        "    zip.extractall('./temp6')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/커피(이름 수정).zip', 'r') as zip:\n",
        "    zip.extractall('./temp7')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/맛스타그램.zip', 'r') as zip:\n",
        "    zip.extractall('./temp8')\n",
        "\n",
        "with ZipFile('/content/drive/MyDrive/5조/데이터/정리 전 데이터/꽃.zip', 'r') as zip:\n",
        "    zip.extractall('./temp9')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fuAyWM2LiQf0"
      },
      "outputs": [],
      "source": [
        "columns = ['이미지id', '음식', '음료수', '성인', '아동', '동물', '네일', '꽃', '기타', '패션', '쓰레기']\n",
        "df_total = pd.read_csv('/content/drive/MyDrive/5조/모델/07.21 객체검출 모델/학습데이터.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uvWN08rBYlW",
        "outputId": "e2045a05-abe2-4280-a332-c83cabaf81f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4000000, 4009458)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# 각 폴더에 있는 파일 맨 앞값과 끝값 확인(필요할 때 사용)\n",
        "image_path1='/content/temp1/'\n",
        "image_path2='/content/temp2/╣▌╖┴╡┐╣░/'\n",
        "image_path3='/content/temp3/└░╛╞/'\n",
        "image_path4='/content/temp4/'\n",
        "image_path5='/content/temp5/'\n",
        "image_path6='/content/temp6/'\n",
        "image_path7='/content/temp7/'\n",
        "image_path8='/content/temp8/'\n",
        "image_path9='/content/temp9/▓╔/'\n",
        "\n",
        "image_data=os.listdir(image_path9)\n",
        "lst=[]\n",
        "for data in image_data:\n",
        "  lst.append(int(data.split(\".\")[0]))\n",
        "lst.sort()\n",
        "lst[0], lst[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wJgCeVEXi3ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d559d1d2-6600-4b82-d79f-c9bb4dfcc13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96116736/96112376 [==============================] - 3s 0us/step\n",
            "96124928/96112376 [==============================] - 3s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# 이미지의 벡터화를 진행하는 코드\n",
        "\n",
        "# Load the inception v3 model\n",
        "model_v3 = InceptionV3(weights='imagenet')\n",
        "\n",
        "# Create a new model, by removing the last layer (output layer) from the inception v3\n",
        "model_new = Model(model_v3.input, model_v3.layers[-2].output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd_LeifVAGyt",
        "outputId": "d4a7de4b-5b52-400d-9df0-515895e27a70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# X_train.shape\n",
        "# (50000, 32, 32, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GhhND7xa_qBv"
      },
      "outputs": [],
      "source": [
        "# Function to encode a given image into a vector of size (2048, )\n",
        "def encode(image_path):\n",
        "    img = keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
        "    # Convert image to numpy array of 3-dimensions\n",
        "    x = img_to_array(img)\n",
        "    # Add one more dimension\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # preprocess the images using preprocess_input() from inception module\n",
        "    x = preprocess_input(x)\n",
        "    fea_vec = model_new.predict(x) # Get the encoding vector for the image\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # reshape from (1, 2048) to (2048, )\n",
        "    return fea_vec   # 이 데이터가 input data로 사용됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DQBIcU0Rkwel"
      },
      "outputs": [],
      "source": [
        "# data=[]\n",
        "# for image in tqdm(df_total['이미지id'].values ):\n",
        "#   if int(image[0])==1:\n",
        "#     path=image_path1+image\n",
        "#     data.append(np.array(encode(path)))\n",
        "#   elif int(image[0])==2:\n",
        "#     path=image_path2+image\n",
        "#     data.append(np.array(encode(path)))\n",
        "#   elif int(image[0])==3:\n",
        "#     path=image_path3+image\n",
        "#     data.append(np.array(encode(path)))\n",
        "#   elif int(image[0])==6:\n",
        "#     if int(image.split('.')[0])>=6028883 and int(image.split('.')[0])<= 6047176:\n",
        "#       path=image_path4+image\n",
        "#       data.append(np.array(encode(path)))\n",
        "#     elif int(image.split('.')[0])>=6000000 and int(image.split('.')[0])<= 6007780:\n",
        "#       path=image_path5+image\n",
        "#       data.append(np.array(encode(path)))\n",
        "#     elif int(image.split('.')[0])>=6007781 and int(image.split('.')[0])<= 6028882:\n",
        "#       path=image_path6+image\n",
        "#       data.append(np.array(encode(path)))\n",
        "#     elif int(image.split('.')[0])>=6056666 and int(image.split('.')[0])<= 6073230:\n",
        "#       path=image_path7+image\n",
        "#       data.append(np.array(encode(path)))\n",
        "#     elif int(image.split('.')[0])>=6047177 and int(image.split('.')[0])<= 6056665:\n",
        "#       path=image_path8+image\n",
        "#       data.append(np.array(encode(path)))\n",
        "#   elif int(image[0])==4:\n",
        "#     path=image_path9+image\n",
        "#     data.append(np.array(encode(path)))\n",
        "\n",
        "data = np.load('/content/drive/MyDrive/5조/모델/07.21 객체검출 모델/training_data.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nFu_E3723801"
      },
      "outputs": [],
      "source": [
        "# 중복 데이터 제거 코드\n",
        "dic = {}\n",
        "for index, i in enumerate(data):\n",
        "  dic[tuple(i)] = index\n",
        "\n",
        "new_data = []\n",
        "for i in dic.values():\n",
        "  new_data.append(np.array(data[i]))\n",
        "new_data = np.array(new_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YgtZwFNAkdF",
        "outputId": "3938d565-0430-47b6-8b93-2434e8a027e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30767/30767 [18:35<00:00, 27.57it/s]\n"
          ]
        }
      ],
      "source": [
        "data = []\n",
        "for image in tqdm(df_total['이미지id']):\n",
        "  if int(image[0])==1:\n",
        "    path=image_path1+image\n",
        "    img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "    data.append(np.array(img))\n",
        "  elif int(image[0])==2:\n",
        "    path=image_path2+image\n",
        "    img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "    data.append(np.array(img))\n",
        "  elif int(image[0])==3:\n",
        "    path=image_path3+image\n",
        "    img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "    data.append(np.array(img))\n",
        "  elif int(image[0])==6:\n",
        "    if int(image.split('.')[0])>=6028883 and int(image.split('.')[0])<= 6047176:\n",
        "      path=image_path4+image\n",
        "      img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "      data.append(np.array(img))\n",
        "    elif int(image.split('.')[0])>=6000000 and int(image.split('.')[0])<= 6007780:\n",
        "      path=image_path5+image\n",
        "      img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "      data.append(np.array(img))\n",
        "    elif int(image.split('.')[0])>=6007781 and int(image.split('.')[0])<= 6028882:\n",
        "      path=image_path6+image\n",
        "      img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "      data.append(np.array(img))\n",
        "    elif int(image.split('.')[0])>=6056666 and int(image.split('.')[0])<= 6073230:\n",
        "      path=image_path7+image\n",
        "      img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "      data.append(np.array(img))\n",
        "    elif int(image.split('.')[0])>=6047177 and int(image.split('.')[0])<= 6056665:\n",
        "      path=image_path8+image\n",
        "      img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "      data.append(np.array(img))\n",
        "  elif int(image[0])==4:\n",
        "    path=image_path9+image\n",
        "    img = keras.preprocessing.image.load_img(path, target_size=(299, 299))\n",
        "    data.append(np.array(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cNLpPfdTEFsP"
      },
      "outputs": [],
      "source": [
        "new_data = []\n",
        "for index, i in enumerate(data):\n",
        "  if index in dic.values():\n",
        "    new_data.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('before_aug_data', new_data) "
      ],
      "metadata": {
        "id": "JXGbiYJvgRSb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gMHJ2NX42GRY"
      },
      "outputs": [],
      "source": [
        "Y=df_total.iloc[list(dic.values()), 1:].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cmyfc51X-B3J"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test=train_test_split(\n",
        "    new_data, Y, test_size=0.2, random_state=2022\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wXl1ftqO7X4I"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,          # randomly rotate images in the range 15 degree\n",
        "    width_shift_range=0.1,      # randomly shift images horizontally \n",
        "    height_shift_range=0.1,     # randomly shift images vertically\n",
        "    shear_range=0.1,            # 도형의 기울기\n",
        "    zoom_range=[0.8,1.2],       # 0.8~1.2 배 크기로 임의로 확대/축소\n",
        "    horizontal_flip=True,       # randomly flip images\n",
        "    vertical_flip=False,        # randomly flip images\n",
        "    fill_mode='nearest'         # set mode for filling points outside the input boundaries\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnr7lWRE_UwQ"
      },
      "outputs": [],
      "source": [
        "datagen.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRyJVE9AxM9T",
        "outputId": "7a212703-57c9-49a4-f4fe-18ff34f03f29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "295/299 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.6842\n",
            "Epoch 1: val_loss improved from inf to 0.20632, saving model to best_model.h5\n",
            "299/299 [==============================] - 2s 5ms/step - loss: 0.2246 - accuracy: 0.6846 - val_loss: 0.2063 - val_accuracy: 0.7054\n",
            "Epoch 2/100\n",
            "298/299 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.7309\n",
            "Epoch 2: val_loss did not improve from 0.20632\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1914 - accuracy: 0.7308 - val_loss: 0.2070 - val_accuracy: 0.7084\n",
            "Epoch 3/100\n",
            "293/299 [============================>.] - ETA: 0s - loss: 0.1828 - accuracy: 0.7393\n",
            "Epoch 3: val_loss improved from 0.20632 to 0.19818, saving model to best_model.h5\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1831 - accuracy: 0.7393 - val_loss: 0.1982 - val_accuracy: 0.7324\n",
            "Epoch 4/100\n",
            "298/299 [============================>.] - ETA: 0s - loss: 0.1766 - accuracy: 0.7492\n",
            "Epoch 4: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1767 - accuracy: 0.7490 - val_loss: 0.2030 - val_accuracy: 0.7151\n",
            "Epoch 5/100\n",
            "293/299 [============================>.] - ETA: 0s - loss: 0.1710 - accuracy: 0.7562\n",
            "Epoch 5: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1718 - accuracy: 0.7561 - val_loss: 0.2059 - val_accuracy: 0.7218\n",
            "Epoch 6/100\n",
            "298/299 [============================>.] - ETA: 0s - loss: 0.1676 - accuracy: 0.7607\n",
            "Epoch 6: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1676 - accuracy: 0.7607 - val_loss: 0.2028 - val_accuracy: 0.7159\n",
            "Epoch 7/100\n",
            "295/299 [============================>.] - ETA: 0s - loss: 0.1617 - accuracy: 0.7628\n",
            "Epoch 7: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1615 - accuracy: 0.7634 - val_loss: 0.2113 - val_accuracy: 0.7272\n",
            "Epoch 8/100\n",
            "297/299 [============================>.] - ETA: 0s - loss: 0.1561 - accuracy: 0.7737\n",
            "Epoch 8: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1560 - accuracy: 0.7736 - val_loss: 0.2095 - val_accuracy: 0.7236\n",
            "Epoch 9/100\n",
            "299/299 [==============================] - ETA: 0s - loss: 0.1509 - accuracy: 0.7762\n",
            "Epoch 9: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1509 - accuracy: 0.7762 - val_loss: 0.2120 - val_accuracy: 0.7084\n",
            "Epoch 10/100\n",
            "296/299 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.7836\n",
            "Epoch 10: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1445 - accuracy: 0.7837 - val_loss: 0.2170 - val_accuracy: 0.7067\n",
            "Epoch 11/100\n",
            "299/299 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.7932\n",
            "Epoch 11: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1390 - accuracy: 0.7932 - val_loss: 0.2179 - val_accuracy: 0.7098\n",
            "Epoch 12/100\n",
            "299/299 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.8002\n",
            "Epoch 12: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1320 - accuracy: 0.8002 - val_loss: 0.2314 - val_accuracy: 0.7010\n",
            "Epoch 13/100\n",
            "291/299 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.8079\n",
            "Epoch 13: val_loss did not improve from 0.19818\n",
            "299/299 [==============================] - 1s 4ms/step - loss: 0.1262 - accuracy: 0.8075 - val_loss: 0.2563 - val_accuracy: 0.7025\n"
          ]
        }
      ],
      "source": [
        "# 모델 정의\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dense(256, activation='relu')(inputs1)\n",
        "fe2 = Dense(512, activation='relu')(fe1)\n",
        "decoder1 = Dense(256, activation='relu')(fe2)\n",
        "outputs = Dense(8, activation='sigmoid')(decoder1)   # multi label classification 모델이므로 activation이 sigmoid가 되어야 함\n",
        "model = Model(inputs=inputs1, outputs=outputs)\n",
        "\n",
        "# 모델 설정\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model_path='best_model.h5'\n",
        "mc=ModelCheckpoint(model_path, save_best_only=True, verbose=1)\n",
        "es=EarlyStopping(patience=10)\n",
        "\n",
        "# 모델 훈련\n",
        "hist=model.fit(\n",
        "    X_train, Y_train, validation_split=0.2, epochs=100, callbacks=[mc,es],  batch_size=64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG1AxdnR13Xp",
        "outputId": "3400b4a9-e6c1-4069-d2a2-e64f3645788b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_score 0.6396652719665272\n",
            "음식 0.9010878661087867\n",
            "음료수 0.916652719665272\n",
            "성인 0.8938912133891214\n",
            "아동 0.9384100418410042\n",
            "동물 0.9626778242677825\n",
            "네일 0.9688702928870293\n",
            "꽃 0.9327196652719665\n",
            "기타 0.938744769874477\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "best_model=load_model('best_model.h5')\n",
        "label=best_model.predict(X_test)\n",
        "\n",
        "y_pred=[]\n",
        "for sample in label:\n",
        "  y_pred.append([1 if i>=0.5 else 0 for i in sample ] )\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "print('total_score', accuracy_score(Y_test, y_pred))\n",
        "\n",
        "real_df=pd.DataFrame(Y_test)\n",
        "pred_df=pd.DataFrame(y_pred)\n",
        "\n",
        "for i in range(len(real_df.columns)):\n",
        "  print(columns[1:][i], accuracy_score( real_df[i].values, pred_df[i].values ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpEjGzAEjvBD"
      },
      "outputs": [],
      "source": [
        "# 중복제거\n",
        "# total_score 0.7979916317991632\n",
        "# 음식 0.9579916317991631\n",
        "# 음료수 0.9554811715481172\n",
        "# 성인 0.9422594142259414\n",
        "# 아동 0.9589958158995816\n",
        "# 동물 0.9866108786610879\n",
        "# 네일 0.9882845188284519\n",
        "# 꽃 0.9638493723849373\n",
        "# 기타 0.9511297071129707"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx4S2Ui_994W"
      },
      "outputs": [],
      "source": [
        "np.save('training_data', new_data) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxQQsI2yAOa7"
      },
      "outputs": [],
      "source": [
        "df_total.to_csv('학습데이터.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "keras_image_captioning_전체데이터.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}